{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9008bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_preferred_device() -> cuda\n",
      "Using device: cuda\n",
      "/home/huqianghui/fluxgym/models/unet/flux1-dev.sft exists: True\n",
      "/home/huqianghui/fluxgym/models/vae/ae.sft exists: True\n",
      "/home/huqianghui/fluxgym/models/clip/clip_l.safetensors exists: True\n",
      "/home/huqianghui/fluxgym/models/clip/t5xxl_fp16.safetensors exists: True\n",
      "/home/huqianghui/fluxgym/outputs/tabtab-food-demo-style-01/tabtab-food-demo-style-01.safetensors exists: True\n"
     ]
    }
   ],
   "source": [
    "# Setup: paths and imports for FLUX.1-dev + LoRA\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Make sd-scripts importable\n",
    "root = Path.cwd().parent  # repository root: .../fluxgym\n",
    "sys.path.append(str(root / \"sd-scripts\"))\n",
    "\n",
    "from library.device_utils import get_preferred_device, init_ipex\n",
    "init_ipex()\n",
    "device = get_preferred_device()\n",
    "\n",
    "# Paths (edit these to your actual checkpoints if different)\n",
    "flux_ckpt = root / \"models\" / \"unet\" / \"flux1-dev.sft\"                 # FLUX.1-dev (UNet/DiT)\n",
    "ae_ckpt   = root / \"models\" / \"vae\" / \"ae.sft\"                         # AutoEncoder\n",
    "clip_ckpt = root / \"models\" / \"clip\" / \"clip_l.safetensors\"           # CLIP-L\n",
    "t5_ckpt   = root / \"models\" / \"clip\" / \"t5xxl_fp16.safetensors\"       # T5 XXL\n",
    "lora_ckpt = root / \"outputs\" / \"tabtab-food-demo-style-01\" / \"tabtab-food-demo-style-01.safetensors\"  # Your LoRA\n",
    "\n",
    "# Dtypes (bf16 helps avoid OOM; keep CLIP and LoRA consistent later)\n",
    "dtype = torch.bfloat16\n",
    "clip_l_dtype = dtype\n",
    "t5xxl_dtype = dtype\n",
    "ae_dtype = dtype\n",
    "flux_dtype = dtype\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "for p in [flux_ckpt, ae_ckpt, clip_ckpt, t5_ckpt, lora_ckpt]:\n",
    "    print(p, \"exists:\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f8cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huqianghui/fluxgym/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2025-09-01 08:02:05 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Building CLIP-L                                                      <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#254\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2025-09-01 08:02:05\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Building CLIP-L                                                      \u001b]8;id=39451;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=10945;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#254\u001b\\\u001b[2m254\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading state dict from                                              <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#350\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">350</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/huqianghui/fluxgym/models/clip/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">clip_l.safetensors</span>              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading state dict from                                              \u001b]8;id=967293;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=924654;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#350\u001b\\\u001b[2m350\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/huqianghui/fluxgym/models/clip/\u001b[0m\u001b[95mclip_l.safetensors\u001b[0m              \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loaded CLIP-L: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">All</span><span style=\"color: #000000; text-decoration-color: #000000\"> keys matched successfully</span><span style=\"font-weight: bold\">&gt;</span>                       <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#353\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">353</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded CLIP-L: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched successfully\u001b[0m\u001b[1m>\u001b[0m                       \u001b]8;id=696061;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=123117;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#353\u001b\\\u001b[2m353\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2025-09-01 08:02:06 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading state dict from                                              <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#405\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">405</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/huqianghui/fluxgym/models/clip/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">t5xxl_fp16.safetensors</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2025-09-01 08:02:06\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading state dict from                                              \u001b]8;id=772813;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=121894;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#405\u001b\\\u001b[2m405\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/huqianghui/fluxgym/models/clip/\u001b[0m\u001b[95mt5xxl_fp16.safetensors\u001b[0m          \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2025-09-01 08:02:07 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loaded T5xxl: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">All</span><span style=\"color: #000000; text-decoration-color: #000000\"> keys matched successfully</span><span style=\"font-weight: bold\">&gt;</span>                        <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#408\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">408</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2025-09-01 08:02:07\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded T5xxl: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched successfully\u001b[0m\u001b[1m>\u001b[0m                        \u001b]8;id=351555;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=517772;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#408\u001b\\\u001b[2m408\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Checking the state dict: Diffusers or BFL, dev or schnell             <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Checking the state dict: Diffusers or BFL, dev or schnell             \u001b]8;id=739258;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=960779;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Building Flux model dev from BFL checkpoint                          <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#107\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Building Flux model dev from BFL checkpoint                          \u001b]8;id=971912;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=423466;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#107\u001b\\\u001b[2m107\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading state dict from                                              <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#124\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/huqianghui/fluxgym/models/unet/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">flux1-dev.sft</span>                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading state dict from                                              \u001b]8;id=257593;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=305668;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#124\u001b\\\u001b[2m124\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/huqianghui/fluxgym/models/unet/\u001b[0m\u001b[95mflux1-dev.sft\u001b[0m                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2025-09-01 08:02:11 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loaded Flux: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">All</span><span style=\"color: #000000; text-decoration-color: #000000\"> keys matched successfully</span><span style=\"font-weight: bold\">&gt;</span>                         <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#143\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2025-09-01 08:02:11\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded Flux: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched successfully\u001b[0m\u001b[1m>\u001b[0m                         \u001b]8;id=55090;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=980744;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#143\u001b\\\u001b[2m143\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_schnell: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Building AutoEncoder                                                 <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#179\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">179</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Building AutoEncoder                                                 \u001b]8;id=28450;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99100;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#179\u001b\\\u001b[2m179\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading state dict from <span style=\"color: #800080; text-decoration-color: #800080\">/home/huqianghui/fluxgym/models/vae/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ae.sft</span>   <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#184\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading state dict from \u001b[35m/home/huqianghui/fluxgym/models/vae/\u001b[0m\u001b[95mae.sft\u001b[0m   \u001b]8;id=308580;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=633593;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#184\u001b\\\u001b[2m184\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loaded AE: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">All</span><span style=\"color: #000000; text-decoration-color: #000000\"> keys matched successfully</span><span style=\"font-weight: bold\">&gt;</span>                           <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flux_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#187\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded AE: \u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched successfully\u001b[0m\u001b[1m>\u001b[0m                           \u001b]8;id=768343;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py\u001b\\\u001b[2mflux_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=302306;file:///home/huqianghui/fluxgym/sd-scripts/library/flux_utils.py#187\u001b\\\u001b[2m187\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base components.\n"
     ]
    }
   ],
   "source": [
    "# Load base FLUX components\n",
    "from safetensors.torch import load_file\n",
    "from library import flux_utils, strategy_flux\n",
    "\n",
    "# Load text encoders\n",
    "clip_l = flux_utils.load_clip_l(str(clip_ckpt), clip_l_dtype, device)\n",
    "clip_l.eval()\n",
    "\n",
    "t5xxl = flux_utils.load_t5xxl(str(t5_ckpt), t5xxl_dtype, device)\n",
    "t5xxl.eval()\n",
    "\n",
    "# Load the DiT (FLUX.1 dev) and tell if it's schnell or dev\n",
    "is_schnell, flux_model = flux_utils.load_flow_model(str(flux_ckpt), flux_dtype, device, model_type=\"flux\")\n",
    "flux_model.eval()\n",
    "print(\"is_schnell:\", is_schnell)\n",
    "\n",
    "# Load AE\n",
    "ae = flux_utils.load_ae(str(ae_ckpt), ae_dtype, device)\n",
    "ae.eval()\n",
    "\n",
    "# Tokenization/encoding strategies\n",
    "from library import strategy_flux as strat\n",
    "max_len = 256 if is_schnell else 512\n",
    "tokenize_strategy = strat.FluxTokenizeStrategy(max_len)\n",
    "encoding_strategy = strat.FluxTextEncodingStrategy()\n",
    "\n",
    "print(\"Loaded base components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09255478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2025-09-01 08:02:15 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> create LoRA network from weights                                      <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#770\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m2025-09-01 08:02:15\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA network from weights                                      \u001b]8;id=555109;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=49089;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#770\u001b\\\u001b[2m770\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> train all blocks only                                                 <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#789\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">789</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m train all blocks only                                                 \u001b]8;id=308829;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=713093;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#789\u001b\\\u001b[2m789\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> create LoRA for Text Encoder <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:                                       <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#939\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">939</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for Text Encoder \u001b[1;36m1\u001b[0m:                                       \u001b]8;id=194355;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=557253;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#939\u001b\\\u001b[2m939\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> create LoRA for Text Encoder <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72</span> modules.                           <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#942\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">942</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for Text Encoder \u001b[1;36m1\u001b[0m: \u001b[1;36m72\u001b[0m modules.                           \u001b]8;id=791834;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=179338;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#942\u001b\\\u001b[2m942\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> create LoRA for FLUX all blocks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span> modules.                         <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#963\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">963</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m create LoRA for FLUX all blocks: \u001b[1;36m304\u001b[0m modules.                         \u001b]8;id=812633;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=396299;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#963\u001b\\\u001b[2m963\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> enable LoRA for text encoder: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72</span> modules                             <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#1138\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1138</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m enable LoRA for text encoder: \u001b[1;36m72\u001b[0m modules                             \u001b]8;id=424565;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=111325;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#1138\u001b\\\u001b[2m1138\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> enable LoRA for U-Net: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span> modules                                   <a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora_flux.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#1143\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1143</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m enable LoRA for U-Net: \u001b[1;36m304\u001b[0m modules                                   \u001b]8;id=226687;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py\u001b\\\u001b[2mlora_flux.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=502856;file:///home/huqianghui/fluxgym/sd-scripts/networks/lora_flux.py#1143\u001b\\\u001b[2m1143\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA attached. Casted CLIP-L & LoRA to torch.bfloat16 on cuda. <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Apply your LoRA to the loaded base\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Ensure sd-scripts is importable\n",
    "root = Path.cwd().parent\n",
    "if str(root / \"sd-scripts\") not in sys.path:\n",
    "    sys.path.append(str(root / \"sd-scripts\"))\n",
    "\n",
    "from networks import lora_flux, oft_flux\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "# Multiplier controls LoRA strength\n",
    "def load_and_apply_lora(lora_path: str, multiplier: float = 1.0, merge: bool = False):\n",
    "    sd = load_safetensors(lora_path)\n",
    "    # Detect LoRA vs OFT\n",
    "    is_lora = any(k.startswith(\"lora\") for k in sd.keys())\n",
    "    module = lora_flux if is_lora else oft_flux\n",
    "\n",
    "    lora_model, _ = module.create_network_from_weights(\n",
    "        multiplier, None, ae, [clip_l, t5xxl], flux_model, sd, True\n",
    "    )\n",
    "\n",
    "    if merge:\n",
    "        lora_model.merge_to([clip_l, t5xxl], flux_model, sd)\n",
    "        print(\"LoRA merged into base model.\")\n",
    "        return None\n",
    "    else:\n",
    "        lora_model.apply_to([clip_l, t5xxl], flux_model)\n",
    "        info = lora_model.load_state_dict(sd, strict=True)\n",
    "        lora_model.eval()\n",
    "        # Keep CLIP and LoRA on GPU and same dtype to avoid matmul/device mismatch\n",
    "        clip_l.to(device=device, dtype=clip_l_dtype)\n",
    "        lora_model.to(device=device, dtype=clip_l_dtype)\n",
    "        print(f\"LoRA attached. Casted CLIP-L & LoRA to {clip_l_dtype} on {device}.\", info)\n",
    "        return lora_model\n",
    "\n",
    "lora_model = load_and_apply_lora(str(lora_ckpt), multiplier=1.0, merge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b98a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule + generate helpers\n",
    "import torch, math, einops\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def time_shift(mu: float, sigma: float, t: torch.Tensor):\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n",
    "\n",
    "\n",
    "def _lin(x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15):\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return lambda x: m * x + b\n",
    "\n",
    "\n",
    "def get_schedule(num_steps: int, image_seq_len: int, shift: bool = True) -> list[float]:\n",
    "    ts = torch.linspace(1.0, 0.0, num_steps + 1, device=device)\n",
    "    if shift and not is_schnell:\n",
    "        mu = _lin()(image_seq_len)\n",
    "        ts = time_shift(mu, 1.0, ts)\n",
    "    return ts.tolist()\n",
    "\n",
    "\n",
    "def _denoise(\n",
    "    model,\n",
    "    img: torch.Tensor,\n",
    "    img_ids: torch.Tensor,\n",
    "    t5_out: Optional[torch.Tensor],\n",
    "    txt_ids: Optional[torch.Tensor],\n",
    "    l_pooled: Optional[torch.Tensor],\n",
    "    timesteps: list[float],\n",
    "    guidance: float,\n",
    "    t5_attn_mask: Optional[torch.Tensor] = None,\n",
    "    neg_t5_out: Optional[torch.Tensor] = None,\n",
    "    neg_l_pooled: Optional[torch.Tensor] = None,\n",
    "    neg_t5_attn_mask: Optional[torch.Tensor] = None,\n",
    "    cfg_scale: Optional[float] = None,\n",
    "):\n",
    "    do_cfg = neg_t5_out is not None and (cfg_scale is not None and cfg_scale != 1.0)\n",
    "    guidance_vec = torch.full((img.shape[0] * (2 if do_cfg else 1),), guidance, device=img.device, dtype=img.dtype)\n",
    "\n",
    "    if do_cfg:\n",
    "        b_img_ids = torch.cat([img_ids, img_ids], dim=0)\n",
    "        b_txt_ids = torch.cat([txt_ids, txt_ids], dim=0) if txt_ids is not None else None\n",
    "        b_txt = torch.cat([neg_t5_out, t5_out], dim=0)\n",
    "        b_vec = torch.cat([neg_l_pooled, l_pooled], dim=0) if l_pooled is not None else None\n",
    "        b_t5_attn_mask = (\n",
    "            torch.cat([neg_t5_attn_mask, t5_attn_mask], dim=0) if (t5_attn_mask is not None and neg_t5_attn_mask is not None) else None\n",
    "        )\n",
    "    else:\n",
    "        b_img_ids = img_ids\n",
    "        b_txt_ids = txt_ids\n",
    "        b_txt = t5_out\n",
    "        b_vec = l_pooled\n",
    "        b_t5_attn_mask = t5_attn_mask\n",
    "\n",
    "    for t_curr, t_prev in zip(timesteps[:-1], timesteps[1:]):\n",
    "        t_vec = torch.full((b_img_ids.shape[0],), t_curr, dtype=img.dtype, device=img.device)\n",
    "        b_img = torch.cat([img, img], dim=0) if do_cfg else img\n",
    "\n",
    "        mod_vectors = model.get_mod_vectors(timesteps=t_vec, guidance=guidance_vec, batch_size=b_img.shape[0])\n",
    "\n",
    "        pred = model(\n",
    "            img=b_img,\n",
    "            img_ids=b_img_ids,\n",
    "            txt=b_txt,\n",
    "            txt_ids=b_txt_ids,\n",
    "            y=b_vec,\n",
    "            timesteps=t_vec,\n",
    "            guidance=guidance_vec,\n",
    "            txt_attention_mask=b_t5_attn_mask,\n",
    "            mod_vectors=mod_vectors,\n",
    "        )\n",
    "\n",
    "        if do_cfg:\n",
    "            pred_uncond, pred = torch.chunk(pred, 2, dim=0)\n",
    "            pred = pred_uncond + cfg_scale * (pred - pred_uncond)\n",
    "\n",
    "        img = img + (t_prev - t_curr) * pred\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "essential_channels, ph, pw = 16, 2, 2\n",
    "\n",
    "\n",
    "def generate(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"\",\n",
    "    width: int = 768,\n",
    "    height: int = 768,\n",
    "    steps: int = 20,\n",
    "    seed: Optional[int] = 1234,\n",
    "    guidance: float = 3.5,\n",
    "    cfg_scale: float = 1.0,\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Ensure multiples of 16 for Flux AE packing\n",
    "    height = max(64, height - height % 16)\n",
    "    width = max(64, width - width % 16)\n",
    "\n",
    "    g = torch.Generator(device=device)\n",
    "    if seed is not None:\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "    # Encode text using strategies (encoders may be on CPU); move outputs to Flux device/dtype\n",
    "    tokens = tokenize_strategy.tokenize(prompt)\n",
    "    l_pooled, t5_out, txt_ids, t5_attn_mask = encoding_strategy.encode_tokens(tokenize_strategy, [clip_l, t5xxl], tokens)\n",
    "\n",
    "    # Move encoded outputs to Flux device/dtype\n",
    "    if l_pooled is not None:\n",
    "        l_pooled = l_pooled.to(device=device, dtype=flux_dtype)\n",
    "    t5_out = t5_out.to(device=device, dtype=flux_dtype)\n",
    "    # Some impls return zeros for txt_ids; cast to flux dtype on device for safety\n",
    "    if txt_ids is not None:\n",
    "        try:\n",
    "            txt_ids = txt_ids.to(device=device, dtype=flux_dtype)\n",
    "        except Exception:\n",
    "            txt_ids = txt_ids.to(device=device)\n",
    "\n",
    "    use_mask = bool(getattr(encoding_strategy, \"apply_t5_attn_mask\", False))\n",
    "\n",
    "    if cfg_scale != 1.0 and negative_prompt is not None:\n",
    "        neg_tokens = tokenize_strategy.tokenize(negative_prompt)\n",
    "        neg_l_pooled, neg_t5_out, _, neg_t5_attn_mask = encoding_strategy.encode_tokens(\n",
    "            tokenize_strategy, [clip_l, t5xxl], neg_tokens\n",
    "        )\n",
    "        if neg_l_pooled is not None:\n",
    "            neg_l_pooled = neg_l_pooled.to(device=device, dtype=flux_dtype)\n",
    "        if neg_t5_out is not None:\n",
    "            neg_t5_out = neg_t5_out.to(device=device, dtype=flux_dtype)\n",
    "    else:\n",
    "        neg_l_pooled = neg_t5_out = neg_t5_attn_mask = None\n",
    "\n",
    "    # Move masks to device and proper dtype (bool works with SDPA)\n",
    "    if use_mask and t5_attn_mask is not None:\n",
    "        t5_attn_mask = t5_attn_mask.to(device, non_blocking=True).bool()\n",
    "    else:\n",
    "        t5_attn_mask = None\n",
    "    if use_mask and neg_t5_attn_mask is not None:\n",
    "        neg_t5_attn_mask = neg_t5_attn_mask.to(device, non_blocking=True).bool()\n",
    "    else:\n",
    "        neg_t5_attn_mask = None\n",
    "\n",
    "    # Prepare latents\n",
    "    H, W = height // 16, width // 16\n",
    "    with torch.autocast(\"cuda\", dtype=flux_dtype), torch.no_grad():\n",
    "        x = torch.randn(1, H * W, essential_channels * ph * pw, device=device, dtype=flux_dtype, generator=g)\n",
    "        img_ids = flux_utils.prepare_img_ids(1, H, W).to(device=device, dtype=flux_dtype)\n",
    "        timesteps = get_schedule(steps, x.shape[1], shift=True)\n",
    "\n",
    "        x = _denoise(\n",
    "            flux_model,\n",
    "            x,\n",
    "            img_ids,\n",
    "            t5_out,\n",
    "            txt_ids,\n",
    "            l_pooled,\n",
    "            timesteps,\n",
    "            guidance,\n",
    "            t5_attn_mask,\n",
    "            neg_t5_out,\n",
    "            neg_l_pooled,\n",
    "            neg_t5_attn_mask,\n",
    "            cfg_scale,\n",
    "        )\n",
    "\n",
    "        # Rearrange to image space for AE\n",
    "        x = einops.rearrange(x, \"b (h w) (cc ph pw) -> b cc (h ph) (w pw)\", h=H, w=W, ph=ph, pw=pw)\n",
    "\n",
    "    # Decode with AE; fallback to CPU on OOM\n",
    "    ae.to(device)\n",
    "    ae_dtype = next(ae.parameters()).dtype\n",
    "    x = x.to(device=device, dtype=ae_dtype)\n",
    "    decode_dtype = ae_dtype if ae_dtype in (torch.float16, torch.bfloat16) else None\n",
    "    try:\n",
    "        if decode_dtype is not None:\n",
    "            with torch.no_grad(), torch.autocast(\"cuda\", dtype=decode_dtype):\n",
    "                x = ae.decode(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                x = ae.decode(x)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            # free some memory and try CPU decode\n",
    "            torch.cuda.empty_cache()\n",
    "            x_cpu = x.float().cpu()\n",
    "            ae_cpu = ae.to(\"cpu\", dtype=torch.float32)\n",
    "            with torch.inference_mode():\n",
    "                x = ae_cpu.decode(x_cpu)\n",
    "            # move back to CUDA tensor for consistency (small tensor after clamp/permute handled on CPU anyway)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    x = x.clamp(-1, 1).permute(0, 2, 3, 1)\n",
    "    img = Image.fromarray((127.5 * (x + 1.0)).float().detach().cpu().numpy().astype(np.uint8)[0])\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df2c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 72 TE LoRA modules to CPU/float32 to match encoders.\n",
      "Encoders (and TE LoRA) moved to CPU. CUDA memory cleared.\n",
      "Encoders (and TE LoRA) moved to CPU. CUDA memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Offload text encoders to CPU to save VRAM before generation\n",
    "import torch, gc\n",
    "\n",
    "# Move encoders to CPU\n",
    "if 'clip_l' in globals() and clip_l is not None:\n",
    "    clip_l.to('cpu', dtype=torch.float32)\n",
    "    clip_l.eval()\n",
    "if 't5xxl' in globals() and t5xxl is not None:\n",
    "    t5xxl.to('cpu', dtype=torch.float32)\n",
    "    t5xxl.eval()\n",
    "\n",
    "# Also move any text-encoder LoRA modules to CPU/float32 so devices match\n",
    "try:\n",
    "    moved = 0\n",
    "    if 'lora_model' in globals() and lora_model is not None:\n",
    "        for l in getattr(lora_model, 'text_encoder_loras', []):\n",
    "            l.to('cpu', dtype=torch.float32).eval()\n",
    "            for p in l.parameters():\n",
    "                p.data = p.data.to(dtype=torch.float32)\n",
    "            moved += 1\n",
    "    if moved:\n",
    "        print(f\"Moved {moved} TE LoRA modules to CPU/float32 to match encoders.\")\n",
    "except Exception as e:\n",
    "    print('warn: failed moving TE LoRA to CPU:', e)\n",
    "\n",
    "# Optionally reduce T5 token length to speed up CPU encoding\n",
    "try:\n",
    "    from library.strategy_flux import FluxTokenizeStrategy\n",
    "    tokenize_strategy = FluxTokenizeStrategy(t5xxl_max_length=256)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Free up GPU memory held by encoders\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('Encoders (and TE LoRA) moved to CPU. CUDA memory cleared.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3c7470",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma colorful bowl of noodles, food photography, 50mm, depth of field\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1234\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display(img)\n",
      "Cell \u001b[0;32mIn[4], line 194\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, negative_prompt, width, height, steps, seed, guidance, cfg_scale)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43m(\u001b[49m\u001b[38;5;241;43m127.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# Quick test: call generate() and display the result\n",
    "from IPython.display import display\n",
    "\n",
    "prompt = \"a colorful bowl of noodles, food photography, 50mm, depth of field\"\n",
    "img = generate(prompt, width=512, height=512, steps=12, guidance=3.5, cfg_scale=1.0, seed=1234)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d3f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and LoRA moved to CUDA with bfloat16.\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Move everything to CUDA (bf16) for single-device execution\n",
    "import torch\n",
    "\n",
    "# Target device/dtype\n",
    "_cuda = torch.device('cuda')\n",
    "_dt = torch.bfloat16\n",
    "\n",
    "# Core models\n",
    "if 'flux_model' in globals() and flux_model is not None:\n",
    "    flux_model.to(device=_cuda, dtype=_dt).eval()\n",
    "if 'ae' in globals() and ae is not None:\n",
    "    ae.to(device=_cuda, dtype=_dt).eval()\n",
    "if 'clip_l' in globals() and clip_l is not None:\n",
    "    clip_l.to(device=_cuda, dtype=_dt).eval()\n",
    "if 't5xxl' in globals() and t5xxl is not None:\n",
    "    t5xxl.to(device=_cuda, dtype=_dt).eval()\n",
    "\n",
    "# LoRA modules\n",
    "if 'lora_model' in globals() and lora_model is not None:\n",
    "    lora_model.to(device=_cuda, dtype=_dt).eval()\n",
    "    # Ensure nested LoRA submodules are on the same device/dtype\n",
    "    for p in lora_model.parameters():\n",
    "        p.data = p.data.to(device=_cuda, dtype=_dt)\n",
    "\n",
    "# Update globals\n",
    "device = _cuda\n",
    "clip_l_dtype = _dt\n",
    "t5xxl_dtype = _dt\n",
    "ae_dtype = _dt\n",
    "flux_dtype = _dt\n",
    "\n",
    "import gc\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "print('All models and LoRA moved to CUDA with bfloat16 (optional step).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
